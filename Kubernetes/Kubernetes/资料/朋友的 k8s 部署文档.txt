一、集群环境		
	Harbor自行安装	
		Vip: 192.168.55.250
		192.168.55.8		harbor
		192.168.55.241		k8s-master1
		192.168.55.242		k8s-master2
		192.168.55.243		k8s-node1
		192.168.55.244		k8s-node2	
		192.168.55.245		k8s-etcd-master 
		192.168.55.246		k8s-etcd1
		192.168.55.247		k8s-etcd2 
		192.168.55.248		k8s-keepalived1+haproxy1
		192.168.55.249		k8s-keepalived2+haproxy2
安装前准备：Centos7.4	
	1：安装docker
		yum install docker
	2: 关闭所有节点的selinux
		setenforce 0
	3: 关闭所有节点的防火墙
		systemctl disable firewalld 
		systemctl stop firewalld

注意：
	1： 由于启用了 TLS 双向认证、RBAC 授权等严格的安全机制，建议从头开始部署，而不要从中间开始，否则可能会认证、授权等失败！
	2： 部署过程中需要有很多证书的操作，请大家耐心操作
	3： 该部署操作仅是搭建成了一个可用 kubernetes 集群，而很多地方还需要进行优化
	
	CA:	验证证书,可以设置service或client是否可以用该CA验证证书
	CA证书(ca.pem)：CA会验证请求中CA字段的用户名和O字段的用户所属组来验证CA证书的合法性
	Kubernetes证书：etcd和master集群使用该证书以此来验证自身的合法性
	admin证书：使用该证书访问 kube-apiserver，并且获得相应的权限
	kube-proxy 证书：调用 kube-apiserver Proxy 相关 API 的权限
	
	
	
二、 创建TLS证书和秘钥
	kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
	生成的 CA 证书和秘钥文件如下：
			ca-key.pem
			ca.pem
			kubernetes-key.pem
			kubernetes.pem
			kube-proxy.pem
			kube-proxy-key.pem
			admin.pem
			admin-key.pem
	使用证书的组件如下：
			etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem；
			kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem；
			kubelet：使用 ca.pem；
			kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem；
			kubectl：使用 ca.pem、admin-key.pem、admin.pem；
			kube-controller-manager：使用 ca-key.pem、ca.pem
			注意： 以下操作都在 192.168.55.241 主机上执行，然后分发到集群所有主机，证书只需要创建一次即可，以后在向集群中添加新节点时只要将 /etc/kubernetes/ 目录下的证书拷贝到新节点上即可。

	1：安装CFSSL
			直接使用二进制源码包安装
			wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
			chmod +x cfssl_linux-amd64
			mv cfssl_linux-amd64 /usr/bin/cfssl

			wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
			chmod +x cfssljson_linux-amd64
			mv cfssljson_linux-amd64 /usr/bin/cfssljson

			wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
			chmod +x cfssl-certinfo_linux-amd64
			mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo

	2：创建 CA
			(1) 创建 CA 配置文件
				mkdir /root/ssl
				cd /root/ssl
				cat > ca-config.json << EOF
				{
				  "signing": {
					"default": {
					  "expiry": "87600h"
					},
					"profiles": {
					  "kubernetes": {
						"usages": [
							"signing",
							"key encipherment",
							"server auth",
							"client auth"
						],
						"expiry": "87600h"
					  }
					}
				  }
				}
				EOF
			说明：	ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 - profile；
					signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；
					server auth：表示client可以用该 CA 对server提供的证书进行验证；
					client auth：表示server可以用该CA对client提供的证书进行验证；
			(2) 创建 CA 证书签名请求

				cat > ca-csr.json << EOF
				{
				  "CN": "kubernetes",
				  "key": {
					"algo": "rsa",
					"size": 2048
				  },
				  "names": [
					{
					  "C": "CN",
					  "ST": "BeiJing",
					  "L": "BeiJing",
					  "O": "k8s",
					  "OU": "System"
					}
				  ]
				}
				EOF	
			说明：	"CN"：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；
					"O"：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；		
			(3) 生成 CA 证书和私钥
				cfssl gencert -initca ca-csr.json | cfssljson -bare ca
				ls ca*
				ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
									
	3:  创建 kubernetes 证书
		(1) 创建 kubernetes 证书签名请求文件 kubernetes-csr.json：
		
				cat > kubernetes-csr.json << EOF
				{
					"CN": "kubernetes",
					"hosts": [
					  "127.0.0.1",
					  "192.168.55.7",
					  "192.168.55.250",
					  "192.168.55.241",
					  "192.168.55.242",
					  "192.168.55.245",
					  "192.168.55.246",
					  "192.168.55.247",
					  "10.254.0.1",
					  "kubernetes",
					  "kubernetes.default",
					  "kubernetes.default.svc",
					  "kubernetes.default.svc.cluster",
					  "kubernetes.default.svc.cluster.local"
					],
					"key": {
						"algo": "rsa",
						"size": 2048
					},
					"names": [
						{
							"C": "CN",
							"ST": "BeiJing",
							"L": "BeiJing",
							"O": "k8s",
							"OU": "System"
						}
					]
				}
				EOF
			说明：	如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1）。
					以上节点的IP也可以更换为主机名。
		
		(2) 生成 kubernetes 证书和私钥
			cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
			ls kubernetes*
			kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
	4:	创建 admin 证书
		(1):	创建 admin 证书签名请求文件 admin-csr.json：
				cat > admin-csr.json << EOF
				{
				  "CN": "admin",
				  "hosts": [],
				  "key": {
					"algo": "rsa",
					"size": 2048
				  },
				  "names": [
					{
					  "C": "CN",
					  "ST": "BeiJing",
					  "L": "BeiJing",
					  "O": "system:masters",
					  "OU": "System"
					}
				  ]
				}
				EOF
			说明： 	后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；
					kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；
					O 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；
					这个admin 证书，是将来生成管理员用的kube config 配置文件用的，现在我们一般建议使用RBAC 来对kubernetes 进行角色权限控制， kubernetes 将证书中的CN 字段 作为User， O 字段作为 Group。
			
		(2):	生成 admin 证书和私钥
				cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json|cfssljson -bare admin
				ls admin*
				admin.csr  admin-csr.json  admin-key.pem  admin.pem

	5:	创建 kube-proxy 证书
		(1)	创建 kube-proxy 证书签名请求文件 kube-proxy-csr.json：
				cat > kube-proxy-csr.json << EOF
				{
				  "CN": "system:kube-proxy",
				  "hosts": [],
				  "key": {
					"algo": "rsa",
					"size": 2048
				  },
				  "names": [
					{
					  "C": "CN",
					  "ST": "BeiJing",
					  "L": "BeiJing",
					  "O": "k8s",
					  "OU": "System"
					}
				  ]
				}
				EOF

			CN 指定该证书的 User 为 system:kube-proxy；
			kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；

		(2)	生成 kube-proxy 客户端证书和私钥
				cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
				ls kube-proxy*
				kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
		(3)	校验证书
			使用 opsnssl 命令
				openssl x509  -noout -text -in  kubernetes.pem
			确认 Issuer 字段的内容和 ca-csr.json 一致；
			确认 Subject 字段的内容和 kubernetes-csr.json 一致；
			确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；
			确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致；
		
			使用 cfssl-certinfo 命令
				cfssl-certinfo -cert kubernetes.pem
	6:分发证书
		将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用；
			mkdir -p /etc/kubernetes/ssl
			cp *.pem /etc/kubernetes/ssl

三、安装kubectl命令行工具
	在master节点安装即可
	1：下载kubectl
		wget https://dl.k8s.io/v1.9.2/kubernetes-client-linux-amd64.tar.gz
		tar -xzvf kubernetes-client-linux-amd64.tar.gz
		cp kubernetes/client/bin/kube* /usr/bin/
		chmod a+x /usr/bin/kube*
	2：创建 kubectl kubeconfig 文件
		export KUBE_APISERVER="https://192.168.55.250:6443"
		
		# 设置集群参数
		kubectl config set-cluster kubernetes \
		  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
		  --embed-certs=true \
		  --server=${KUBE_APISERVER}
		  
		# 设置客户端认证参数
		kubectl config set-credentials admin \
		  --client-certificate=/etc/kubernetes/ssl/admin.pem \
		  --embed-certs=true \
		  --client-key=/etc/kubernetes/ssl/admin-key.pem
		  
		# 设置上下文参数
		kubectl config set-context kubernetes \
		  --cluster=kubernetes \
		  --user=admin
		  
		# 设置默认上下文
		kubectl config use-context kubernetes
	说明：	admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；
			生成的 kubeconfig 被保存到 ~/.kube/config 文件；
			注意： ~/.kube/config文件拥有对该集群的最高权限，请妥善保管。如果node节点上需要使用kubelet工具，只需将此文件拷贝过去。		
四、创建 kubeconfig 文件
		kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；
		kuberetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书；
				使用 TLS Bootstrapping 方式进行授权，由 apiserver 自动给符合条件的 Node 发送证书来授权加入集群。主要做法是 kubelet 启动时，向 kube-apiserver 传送 TLS Bootstrapping 请求，而 kube-apiserver 验证 kubelet 请求的 token 是否与设定的一样，若一样就自动产生 kubelet 证书与密钥。
		以下操作只需要在 master1： 192.168.55.241 节点上执行，生成的 *.kubeconfig 文件可以直接拷贝到其他节点的 /etc/kubernetes 目录下。
	1：创建 TLS Bootstrapping Token
			Token可以是任意的包含128 bit的字符串，可以使用安全的随机数发生器生成。
			export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
			cat > token.csv <<EOF
			${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
			EOF		
		注意： 	请检查 token.csv 文件，确认其中的 ${BOOTSTRAP_TOKEN} 环境变量已经被真实的值替换。 **BOOTSTRAP_TOKEN ** 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：
					更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需； 重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下； 重启 kube-apiserver 和 kubelet 进程； 重新 approve kubelet 的 csr 请求；	
			cp token.csv /etc/kubernetes/
	2：创建 kubelet bootstrapping kubeconfig 文件
			cd /etc/kubernetes
			export KUBE_APISERVER="https://192.168.55.250:6443"
			# 设置集群参数
			kubectl config set-cluster kubernetes \
			  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
			  --embed-certs=true \
			  --server=${KUBE_APISERVER} \
			  --kubeconfig=bootstrap.kubeconfig

			# 设置客户端认证参数
			kubectl config set-credentials kubelet-bootstrap \
			  --token=${BOOTSTRAP_TOKEN} \
			  --kubeconfig=bootstrap.kubeconfig

			# 设置上下文参数
			kubectl config set-context default \
			  --cluster=kubernetes \
			  --user=kubelet-bootstrap \
			  --kubeconfig=bootstrap.kubeconfig

			# 设置默认上下文
			kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
		--embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；
		设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成；
	3：创建 kube-proxy kubeconfig 文件
			export KUBE_APISERVER="https://192.168.55.250:6443"
			# 设置集群参数
			kubectl config set-cluster kubernetes \
			  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
			  --embed-certs=true \
			  --server=${KUBE_APISERVER} \
			  --kubeconfig=kube-proxy.kubeconfig
			  
			# 设置客户端认证参数
			kubectl config set-credentials kube-proxy \
			  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
			  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
			  --embed-certs=true \
			  --kubeconfig=kube-proxy.kubeconfig
			  
			# 设置上下文参数
			kubectl config set-context default \
			  --cluster=kubernetes \
			  --user=kube-proxy \
			  --kubeconfig=kube-proxy.kubeconfig
			  
			# 设置默认上下文
			kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
		设置集群参数和客户端认证参数时 --embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；
		kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；
	4：分发 kubeconfig 文件
		将两个 kubeconfig 文件分发到所有 节点机器的 /etc/kubernetes/ 目录

五、创建高可用 etcd 集群
	1：	TLS 认证文件
		需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
			ls /etc/kubernetes/ssl/*.pem
			admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
		kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；
	2：安装etcd
		(1)	下载etcd
			wget https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz
			tar -xvf etcd-v3.1.5-linux-amd64.tar.gz
			mv etcd-v3.1.5-linux-amd64/etcd* /usr/sbin
		(2)	创建 etcd 的 systemd unit 文件
			vi /usr/lib/systemd/system/etcd.service，内容如下。注意替换IP地址为你自己的etcd集群的主机IP
				[Unit]
				Description=Etcd Server
				After=network.target
				After=network-online.target
				Wants=network-online.target
				Documentation=https://github.com/coreos

				[Service]
				Type=notify
				WorkingDirectory=/var/lib/etcd/
				EnvironmentFile=-/etc/etcd/etcd.conf
				ExecStart=/usr/sbin/etcd \
				  --name ${ETCD_NAME} \
				  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
				  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
				  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
				  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
				  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
				  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
				  --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
				  --listen-peer-urls ${ETCD_LISTEN_PEER_URLS} \
				  --listen-client-urls ${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
				  --advertise-client-urls ${ETCD_ADVERTISE_CLIENT_URLS} \
				  --initial-cluster-token ${ETCD_INITIAL_CLUSTER_TOKEN} \
				  --initial-cluster infra1=https://192.168.55.245:2380,infra2=https://192.168.55.246:2380,infra3=https://192.168.55.247:2380 \
				  --initial-cluster-state new \
				  --data-dir=${ETCD_DATA_DIR}
				Restart=on-failure
				RestartSec=5
				LimitNOFILE=65536

				[Install]
				WantedBy=multi-user.target

		说明：	指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这个目录 mkdir -p /var/lib/etcd，否则启动服务的时候会报错“Failed at step CHDIR spawning /usr/bin/etcd: No such file or directory”；
				为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；
				创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的IP，否则证书校验会出错；
				--initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中；
		(3) 环境变量配置文件 vi /etc/etcd/etcd.conf
				# [member]
				ETCD_NAME=infra1
				ETCD_DATA_DIR="/var/lib/etcd"
				ETCD_LISTEN_PEER_URLS="https://192.168.223.201:2380"
				ETCD_LISTEN_CLIENT_URLS="https://192.168.223.201:2379"

				#[cluster]
				ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.223.201:2380"
				ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
				ETCD_ADVERTISE_CLIENT_URLS="https://192.168.223.201:2379"
			这是192.168.223.201节点的配置，其他两个etcd节点只要将上面的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的infra1/2/3。
		(4)	启动 etcd 服务
				systemctl daemon-reload
				systemctl enable etcd
				systemctl start etcd
				systemctl status etcd
		在所有的 kubernetes master 节点重复上面的步骤，直到所有机器的 etcd 服务都已启动
		
		(5)	验证服务
				etcdctl \
				  --ca-file=/etc/kubernetes/ssl/ca.pem \
				  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
				  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
				  cluster-health
			结果最后一行为 cluster is healthy 时表示集群服务正常。

六、部署 haproxy+keepalived
	1：安装 haproxy+keepalived
		yum install -y haproxy keepalived
	2：配置 keepalived
		节点1 192.168.55.248 配置文件 vi /etc/keepalived/keepalived.conf
			! Configuration File for keepalived  
			global_defs {  
				notification_email {   
					test@sina.com   
				}   
				notification_email_from admin@test.com  
				smtp_server 127.0.0.1  
				smtp_connect_timeout 30  
				router_id LVS_MASTER  
			}  

			vrrp_script check_haproxy {
				script "/etc/keepalived/check_haproxy.sh"
				interval 3
			}  

			vrrp_instance VI_1 {  
				state MASTER  # 如果配置主从，从服务器改为BACKUP即可
				interface ens33
				virtual_router_id 60  
				priority 100  # 从服务器设置小于100的数即可
				advert_int 1  
				authentication {  
					auth_type PASS  
					auth_pass 1111  
				}  
				virtual_ipaddress {  
					192.168.55.250/24
				}

				track_script {   
					check_haproxy
				}
			}
		
		节点2 192.168.55.249 配置文件 vi /etc/keepalived/keepalived.conf
			! Configuration File for keepalived  
			global_defs {  
				notification_email {   
					test@sina.com   
				}   
				notification_email_from admin@test.com  
				smtp_server 127.0.0.1  
				smtp_connect_timeout 30  
				router_id LVS_MASTER  
			}  

			vrrp_script check_haproxy {
				script "/etc/keepalived/check_haproxy.sh"
				interval 3
			}  

			vrrp_instance VI_1 {  
				state BACKUP  # 如果配置主从，从服务器改为BACKUP即可
				interface ens33
				virtual_router_id 60  
				priority 90  # 从服务器设置小于100的数即可
				advert_int 1  
				authentication {  
					auth_type PASS  
					auth_pass 1111  
				}  
				virtual_ipaddress {  
					192.168.55.250/24
				}

				track_script {   
					check_haproxy
				}
			}		
		
		检测脚本 vi /etc/keepalived/check_haproxy.sh
			#!/bin/bash
			flag=$(systemctl status haproxy &> /dev/null;echo $?)

			if [[ $flag != 0 ]];then
					echo "haproxy is down,close the keepalived"
					systemctl stop keepalived
			fi
		修改keepalived启动文件 vi /usr/lib/systemd/system/keepalived.service 以下部分:
			[Unit]
			Description=LVS and VRRP High Availability Monitor
			After=syslog.target network-online.target haproxy.service
			Requires=haproxy.service
		keepalived配置文件三台主机基本一样，除了state，主节点配置为MASTER，备节点配置BACKUP，优化级参数priority，主节点设置最高，备节点依次递减
		自定义的检测脚本作用是检测本机haproxy服务状态，如果不正常就停止本机keepalived，释放VIP
		这里没有考虑keepalived脑裂的问题，后期可以在脚本中加入相关检测
	3：配置 haproxy
		两台配置一模一样 配置文件 vim /etc/haproxy/haproxy.cfg
			global
				log         127.0.0.1 local2
				chroot      /var/lib/haproxy
				pidfile     /var/run/haproxy.pid
				maxconn     4000
				user        haproxy
				group       haproxy
				daemon
				stats socket /var/lib/haproxy/stats

			defaults
				mode                    tcp
				log                     global
				option                  tcplog
				option                  dontlognull
				option                  redispatch
				retries                 3
				timeout queue           1m
				timeout connect         10s
				timeout client          1m
				timeout server          1m
				timeout check           10s
				maxconn                 3000

			listen stats
				mode   http
				bind :10086
				stats   enable
				stats   uri     /admin?stats
				stats   auth    admin:admin
				stats   admin   if TRUE

			frontend  k8s_http *:8080
				mode      tcp
				maxconn      2000
				default_backend     http_sri

			backend http_sri
				balance      roundrobin
				server s1 192.168.55.241:8080  check inter 10000 fall 2 rise 2 weight 1
				server s2 192.168.55.242:8080  check inter 10000 fall 2 rise 2 weight 1

			frontend  k8s_https *:6443
				mode      tcp
				maxconn      2000
				default_backend     https_sri

			backend https_sri
				balance      roundrobin
				server s1 192.168.55.241:6443  check inter 10000 fall 2 rise 2 weight 1
				server s2 192.168.55.242:6443  check inter 10000 fall 2 rise 2 weight 1
		listen stats定义了haproxy自身状态查看地址，在里面可以看到haproy目前的各种状态
		frontend 定义了前端提供服务的端口等信息
		backend 定义了后端真实服务器的信息		
						
	4：启动 haproxy+keepalived
			2个节点都启动
			systemctl daemon-reload
			systemctl enable haproxy
			systemctl enable keepalived
			systemctl start haproxy
			systemctl start keepalived			
	如果没有什么报错，那应该就可以在主节点 192.168.55.248 上面看到ens33网卡已绑定VIP: 192.168.55.250

七、安装 flannel 网络插件
		所有的 node 节点都需要安装网络插件才能让所有的Pod加入到同一个局域网中，如果想要在master节点上也能访问 pods的ip，master 节点也安装。
	1：安装 flannel
		建议直接使用 yum 安装 flanneld ，除非对版本有特殊需求，默认安装的是0.7.1版本的 flannel 。
		service配置文件 vi /usr/lib/systemd/system/flanneld.service
			[Unit]
			Description=Flanneld overlay address etcd agent
			After=network.target
			After=network-online.target
			Wants=network-online.target
			After=etcd.service
			Before=docker.service

			[Service]
			Type=notify
			EnvironmentFile=/etc/sysconfig/flanneld
			EnvironmentFile=-/etc/sysconfig/docker-network
			ExecStart=/usr/bin/flanneld-start \
			  -etcd-endpoints=${FLANNEL_ETCD_ENDPOINTS} \
			  -etcd-prefix=${FLANNEL_ETCD_PREFIX} \
			  $FLANNEL_OPTIONS
			ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
			Restart=on-failure

			[Install]
			WantedBy=multi-user.target
			RequiredBy=docker.service
		vi /etc/sysconfig/flanneld 配置文件：
			# Flanneld configuration options
			# etcd url location.  Point this to the server where etcd runs
			FLANNEL_ETCD_ENDPOINTS="https://192.168.55.245:2379,https://192.168.55.246:2379,https://192.168.55.247:2379"
			# etcd config key.  This is the configuration key that flannel queries
			# For address range assignment
			FLANNEL_ETCD_PREFIX="/kube-centos/network"
			# Any additional options that you want to pass
			FLANNEL_OPTIONS="-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem"

		注：如果是主机是多网卡，则需要在FLANNEL_OPTIONS中增加指定的外网出口的网卡，例如-iface=eth2
			在etcd中创建网络配置

	2：在etcd中创建网络配置
		执行下面的命令为docker分配IP地址段
			etcdctl --endpoints=https://192.168.55.245:2379,https://192.168.55.246:2379,https://192.168.55.247:2379 \
			  --ca-file=/etc/kubernetes/ssl/ca.pem \
			  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
			  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
			  mkdir /kube-centos/network

			etcdctl --endpoints=https://192.168.55.245:2379,https://192.168.55.246:2379,https://192.168.55.247:2379 \
			  --ca-file=/etc/kubernetes/ssl/ca.pem \
			  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
			  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
			  mk /kube-centos/network/config '{"Network":"172.30.0.0/16","SubnetLen":24,"Backend":{"Type":"host-gw"}}'
		如果你要使用host-gw模式，可以直接将vxlan改成host-gw即可。根据原作者测试，使用host-gw模式时网络性能好一些。

	3：启动flannel服务
			systemctl stop docker
			systemctl daemon-reload
			systemctl enable flanneld
			systemctl start flanneld
			systemctl status flanneld
			systemctl start docker
		注： 启动flannel前，请先停止docker，flannel启动好后，再启动docker。

		现在查询etcd中的内容可以看到：
			etcdctl --endpoints=${ETCD_ENDPOINTS} \
			  --ca-file=/etc/kubernetes/ssl/ca.pem \
			  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
			  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
			  ls /kube-centos/network/subnets
			/kube-centos/network/subnets/172.30.98.0-24
			/kube-centos/network/subnets/172.30.53.0-24

			etcdctl --endpoints=${ETCD_ENDPOINTS} \
			  --ca-file=/etc/kubernetes/ssl/ca.pem \
			  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
			  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
			  get /kube-centos/network/config
			{"Network":"172.30.0.0/16","SubnetLen":24,"Backend":{"Type":"vxlan"}}

			tcdctl --endpoints=${ETCD_ENDPOINTS} \
			  --ca-file=/etc/kubernetes/ssl/ca.pem \
			  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
			  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
			get /kube-centos/network/subnets/172.30.14.0-24
			{"PublicIP":"192.168.55.244","BackendType":"vxlan","BackendData":{"VtepMAC":"46:fe:67:c9:ed:6e"}}
	
		如果可以查看到以上内容证明flannel已经安装完成，并且已经正常分配kubernetes网段

八、部署master节点
			kubernetes master 节点包含的组件：
			kube-apiserver
			kube-scheduler
			kube-controller-manager
			kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；
			同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；
			kube-apiserver 为无状态服务，使用haproxy+keepalived 实现高可用
		
	1：TLS 证书文件
		以下pem证书文件我们在创建TLS证书和秘钥这一步中已经创建过了，token.csv文件在创建kubeconfig文件的时候创建。我们再检查一下。
			cd /etc/kubernetes/ssl
			ls 
			admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
	2：下载二进制文件
		从 CHANGELOG页面 下载 client 或 server tarball 文件 server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件；
			# wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
			wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
			tar -xzvf kubernetes-server-linux-amd64.tar.gz
			cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/bin/
			chmod +x /usr/bin/kube*
					
	3：配置和启动 kube-apiserver
			创建 kube-apiserver的service配置文件
			service配置文件 vi /usr/lib/systemd/system/kube-apiserver.service内容：
					[Unit]
					Description=Kubernetes API Service
					Documentation=https://github.com/GoogleCloudPlatform/kubernetes
					After=network.target
					After=etcd.service

					[Service]
					EnvironmentFile=-/etc/kubernetes/config
					EnvironmentFile=-/etc/kubernetes/apiserver
					ExecStart=/usr/bin/kube-apiserver \
							$KUBE_LOGTOSTDERR \
							$KUBE_LOG_LEVEL \
							$KUBE_ETCD_SERVERS \
							$KUBE_API_ADDRESS \
							$KUBE_API_PORT \
							$KUBELET_PORT \
							$KUBE_ALLOW_PRIV \
							$KUBE_SERVICE_ADDRESSES \
							$KUBE_ADMISSION_CONTROL \
							$KUBE_API_ARGS
					Restart=on-failure
					Type=notify
					LimitNOFILE=65536

					[Install]
					WantedBy=multi-user.target
			vi /etc/kubernetes/config 文件的内容为：
					###
					# kubernetes system config
					#
					# The following values are used to configure various aspects of all
					# kubernetes services, including
					#
					#   kube-apiserver.service
					#   kube-controller-manager.service
					#   kube-scheduler.service
					#   kubelet.service
					#   kube-proxy.service
					# logging to stderr means we get it in the systemd journal
					KUBE_LOGTOSTDERR="--logtostderr=true"

					# journal message level, 0 is debug
					KUBE_LOG_LEVEL="--v=0"

					# Should this cluster be allowed to run privileged docker containers
					KUBE_ALLOW_PRIV="--allow-privileged=true"

					# How the controller-manager, scheduler, and proxy find the apiserver
					#KUBE_MASTER="--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080"
					KUBE_MASTER="--master=http://192.168.55.250:8080"
				注： 该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。KUBE_MASTER 填写 VIP 地址
			apiserver配置文件 vi /etc/kubernetes/apiserver 内容为：
								
					###
					## kubernetes system config
					##
					## The following values are used to configure the kube-apiserver
					##
					#
					## The address on the local server to listen to.
					#KUBE_API_ADDRESS="--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com"
					KUBE_API_ADDRESS="--advertise-address=0.0.0.0 --bind-address=0.0.0.0 --insecure-bind-address=0.0.0.0"
					#
					## The port on the local server to listen on.
					#KUBE_API_PORT="--port=8080"
					#
					## Port minions listen on
					#KUBELET_PORT="--kubelet-port=10250"
					#
					## Comma separated list of nodes in the etcd cluster
					KUBE_ETCD_SERVERS="--etcd-servers=https://192.168.55.245:2379,https://192.168.55.246:2379,https://192.168.55.247:2379"
					#
					## Address range to use for services
					KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
					#
					## default admission control policies
					KUBE_ADMISSION_CONTROL="--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota"
					#
					## Add your own!
					KUBE_API_ARGS="--authorization-mode=Node,RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --enable-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h"
   			Kubernetes 1.9不同点
					对于Kubernetes1.9集群，需要注意配置KUBE_API_ARGS环境变量中的--authorization-mode=Node,RBAC，增加对Node授权的模式，否则将无法注册node。
					--experimental-bootstrap-token-auth Bootstrap Token Authentication在kubernetes 1.9版本已经废弃，参数名称改为--enable-bootstrap-token-auth

			启动kube-apiserver
					systemctl daemon-reload
					systemctl enable kube-apiserver
					systemctl start kube-apiserver
					systemctl status kube-apiserver
	4：配置和启动 kube-controller-manager
			创建 kube-controller-manager的serivce配置文件
			文件路径 vi /usr/lib/systemd/system/kube-controller-manager.service
					[Unit]
					Description=Kubernetes Controller Manager
					Documentation=https://github.com/GoogleCloudPlatform/kubernetes

					[Service]
					EnvironmentFile=-/etc/kubernetes/config
					EnvironmentFile=-/etc/kubernetes/controller-manager
					ExecStart=/usr/bin/kube-controller-manager \
							$KUBE_LOGTOSTDERR \
							$KUBE_LOG_LEVEL \
							$KUBE_MASTER \
							$KUBE_CONTROLLER_MANAGER_ARGS
					Restart=on-failure
					LimitNOFILE=65536

					[Install]
					WantedBy=multi-user.target
			配置文件 vi /etc/kubernetes/controller-manager。
					###
					# The following values are used to configure the kubernetes controller-manager
					# defaults from config and apiserver should be adequate
					# Add your own!
					KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true"
					--service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；
					--cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；
					--root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；
					--address 值必须为 127.0.0.1，kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；
			启动 kube-controller-manager
					systemctl daemon-reload
					systemctl enable kube-controller-manager
					systemctl start kube-controller-manager
					systemctl status kube-controller-manager
			我们启动每个组件后可以通过执行命令 kubectl get componentstatuses，来查看各个组件的状态;
					kubectl get componentstatuses
			注： 目前scheduler未启动，报错是正常的

	5：配置和启动 kube-scheduler
			创建 kube-scheduler的serivce配置文件
			文件路径 vi /usr/lib/systemd/system/kube-scheduler.service
					[Unit]
					Description=Kubernetes Scheduler Plugin
					Documentation=https://github.com/GoogleCloudPlatform/kubernetes

					[Service]
					EnvironmentFile=-/etc/kubernetes/config
					EnvironmentFile=-/etc/kubernetes/scheduler
					ExecStart=/usr/bin/kube-scheduler \
								$KUBE_LOGTOSTDERR \
								$KUBE_LOG_LEVEL \
								$KUBE_MASTER \
								$KUBE_SCHEDULER_ARGS
					Restart=on-failure
					LimitNOFILE=65536

					[Install]
					WantedBy=multi-user.target
			配置文件 vi /etc/kubernetes/scheduler
					###
					# kubernetes scheduler config

					# default config should be adequate

					# Add your own!
					KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"
			
			启动 kube-scheduler
					systemctl daemon-reload
					systemctl enable kube-scheduler
					systemctl start kube-scheduler
					systemctl status kube-scheduler			
			
			验证 master 节点功能
					kubectl get componentstatuses
					NAME                 STATUS    MESSAGE              ERROR
					scheduler            Healthy   ok                   
					controller-manager   Healthy   ok                   
					etcd-0               Healthy   {"health": "true"}   
					etcd-1               Healthy   {"health": "true"}   
					etcd-2               Healthy   {"health": "true"}
					注： 两个master节点安装方式与配置一样

九、部署node节点
		Kubernetes node节点包含如下组件：
				Flanneld：前面有写Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在service配置文件中增加TLS配置，安装过程请参考上一节安装flannel网络插件。
				Docker1.12.5：docker的安装很简单，这里也不说了，但是需要注意docker的配置。
				kubelet：直接用二进制文件安装
				kube-proxy：直接用二进制文件安装
				注意： 每台 node 上都需要安装 flannel，master 节点上选装。
		步骤简介：
				确认在上一步中我们安装配置的网络插件flannel已启动且运行正常
				安装配置docker后启动
				安装配置kubelet、kube-proxy后启动
				验证
		目录和文件：
				我们再检查一下三个节点上，经过前几步操作我们已经创建了如下的证书和配置文件
				cd  /etc/kubernetes/ssl
				ls
				admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
				ls /etc/kubernetes/
				apiserver  bootstrap.kubeconfig  config  controller-manager  kubelet  kube-proxy.kubeconfig  proxy  scheduler  ssl  token.csv
		1：安装配置Docker
				yum安装的flannel，Docker将会读取/run/flannel/subnet.env的环境变量文件作为容器启动参数
				注意： 安装docker-ce-17.12.1.ce版本的rpm包时，需给docker.service额外添加$DOCKER_NETWORK_OPTIONS --exec-opt native.cgroupdriver=systemd
				
				yum方式安装的flannel：
					修改docker的配置文件 vi /usr/lib/systemd/system/docker.service，增加几条环境变量配置
							EnvironmentFile=-/run/flannel/docker
							EnvironmentFile=/run/flannel/subnet.env
							
					/run/flannel/docker文件是flannel启动后自动生成的，其中包含了docker启动时需要的参数。
				yum方式安装的docker：
					修改配置 vi /etc/sysconfig/docker 中OPTIONS参数如下：
							 OPTIONS='--log-driver=json-file --signature-verification=false --insecure-registry 192.168.55.7:80'   ##启用非安全链接(我用了https，所以这一行没有用)
							 OPTIONS='--log-driver=json-file --add-registry harbor.ztwltech.com'   ##这是我自己的配置
		
						注意：192.168.55.7为harbor地址
				修改 vi /etc/sysconfig/docker-storage 如下：
					DOCKER_STORAGE_OPTIONS="--storage-driver overlay "
					修改docker pull源 vi /etc/docker/daemon.json

					{ 
						"registry-mirrors":["https://harbor.ztwltech.com"]
					}
				
				修改 vi /usr/lib/systemd/system/docker.service:

						[Unit]
						Description=Docker Application Container Engine
						Documentation=http://docs.docker.com
						After=network.target rhel-push-plugin.socket registries.service
						Wants=docker-storage-setup.service
						Requires=docker-cleanup.timer

						[Service]
						Type=notify
						NotifyAccess=all
						EnvironmentFile=-/run/containers/registries.conf
						EnvironmentFile=-/run/flannel/docker
						EnvironmentFile=/run/flannel/subnet.env
						EnvironmentFile=-/etc/sysconfig/docker
						EnvironmentFile=-/etc/sysconfig/docker-storage
						EnvironmentFile=-/etc/sysconfig/docker-network
						Environment=GOTRACEBACK=crash
						Environment=DOCKER_HTTP_HOST_COMPAT=1
						Environment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin
						ExecStart=/usr/bin/dockerd-current \
								  --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \
								  --default-runtime=docker-runc \
								  --exec-opt native.cgroupdriver=systemd \
								  --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \
								  --seccomp-profile=/etc/docker/seccomp.json \
								  $OPTIONS \
								  $DOCKER_STORAGE_OPTIONS \
								  $DOCKER_NETWORK_OPTIONS \
								  $ADD_REGISTRY \
								  $BLOCK_REGISTRY \
								  $INSECURE_REGISTRY \
								  $REGISTRIES
						ExecReload=/bin/kill -s HUP $MAINPID
						LimitNOFILE=1048576
						LimitNPROC=1048576
						LimitCORE=infinity
						TimeoutStartSec=0
						Restart=on-abnormal
						MountFlags=slave
						KillMode=process
						[Install]
						WantedBy=multi-user.target
				启动docker
						systemctl reload-daemon
						systemctl enable docker
						systemctl restart docker
						systemctl status docker
				所有node一样的操作

		2：安装和配置kubelet
				相对于kubernetes1.6集群必须进行的配置有： 对于kuberentes1.8集群，必须关闭swap，否则kubelet启动将失败。 修改/etc/fstab将，swap系统注释掉。
				kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)：
						cd /etc/kubernetes
						kubectl create clusterrolebinding kubelet-bootstrap \
						  --clusterrole=system:node-bootstrapper \
						  --user=kubelet-bootstrap	
				--user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了 /etc/kubernetes/bootstrap.kubeconfig 文件；

				
				
				下载最新的kubelet和kube-proxy二进制文件
						wget https://dl.k8s.io/v1.9.2/kubernetes-server-linux-amd64.tar.gz
						tar -xzvf kubernetes-server-linux-amd64.tar.gz
						cp -r kubernetes/server/bin/{kube-proxy,kubelet} /usr/bin/
						chmod +x /usr/bin/kube*

				创建kubelet的service配置文件
						文件位置 vi /usr/lib/systemd/system/kubelet.service
						[Unit]
						Description=Kubernetes Kubelet Server
						Documentation=https://github.com/GoogleCloudPlatform/kubernetes
						After=docker.service
						Requires=docker.service

						[Service]
						WorkingDirectory=/var/lib/kubelet
						EnvironmentFile=-/etc/kubernetes/config
						EnvironmentFile=-/etc/kubernetes/kubelet
						ExecStart=/usr/bin/kubelet \
									$KUBE_LOGTOSTDERR \
									$KUBE_LOG_LEVEL \
									$KUBELET_API_SERVER \
									$KUBELET_ADDRESS \
									$KUBELET_PORT \
									$KUBELET_HOSTNAME \
									$KUBE_ALLOW_PRIV \
									$KUBELET_POD_INFRA_CONTAINER \
									$KUBELET_ARGS
						Restart=on-failure

						[Install]
						WantedBy=multi-user.target

				kubelet的配置文件/etc/kubernetes/kubelet。其中的IP地址更改为你的每台node节点的IP地址。 注意： 在启动kubelet之前，需要先手动创建/var/lib/kubelet目录：mkdir -p /var/lib/kubelet。
				kubelet的配置文件 vi /etc/kubernetes/kubelet	
						## kubernetes kubelet (minion) config
						#
						## The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
						KUBELET_ADDRESS="--address=192.168.55.243"
						#
						## The port for the info server to serve on
						#KUBELET_PORT="--port=10250"
						#
						## You may leave this blank to use the actual hostname
						KUBELET_HOSTNAME="--hostname-override=192.168.55.243"
						#
						## location of the api-server
						## COMMENT THIS ON KUBERNETES 1.8+
						#KUBELET_API_SERVER="--api-servers=http://192.168.55.250:8080"
						#
						## pod infrastructure container
						KUBELET_POD_INFRA_CONTAINER="--pod_infra_container_image=harbor.ztwltech.com/webservice/pause-amd64:3.0"
						#
						## Add your own!
						KUBELET_ARGS="--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice  --allow-privileged=true --fail-swap-on=false --cluster-dns=10.254.0.2 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local --hairpin-mode promiscuous-bridge --serialize-image-pulls=false"
				
				说明：	1：如果使用systemd方式启动，则需要额外增加两个参数--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice
						2: --experimental-bootstrap-kubeconfig 在1.9版本已经变成了--bootstrap-kubeconfig
						3：--address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；
						4：如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；
						5："--cgroup-driver 配置成 systemd，不要使用cgroup，否则在 CentOS 系统中 kubelet 将启动失败（保持docker和kubelet中的cgroup driver配置一致即可，不一定非使用systemd）。
						6：--experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；
						7：管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 --kubeconfig 文件；
						8：建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;
						9：--cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；
						10：--cluster-domain 指定 pod 启动时 /etc/resolve.conf 文件中的 search domain ，起初我们将其配置成了 cluster.local.，这样在解析 service 的 DNS 名称时是正常的，可是在解析 headless service 中的 FQDN pod name 的时候却错误，因此我们将其修改为 cluster.local，去掉嘴后面的 ”点号“ 就可以解决该问题
						11：--kubeconfig=/etc/kubernetes/kubelet.kubeconfig中指定的kubelet.kubeconfig文件在第一次启动kubelet之前并不存在，请看下文，当通过CSR请求后会自动生成kubelet.kubeconfig文件，如果你的节点上已经生成了~/.kube/config文件，你可以将该文件拷贝到该路径下，并重命名为kubelet.kubeconfig，所有node节点可以共用同一个kubelet.kubeconfig文件，这样新添加的节点就不需要再创建CSR请求就能自动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使用kubectl --kubeconfig命令操作集群时，只要使用~/.kube/config文件就可以通过权限认证，因为这里面已经有认证信息并认为你是admin用户，对集群拥有所有权限。
						12：KUBELET_POD_INFRA_CONTAINER 是基础pod镜像容器，这里我用的是私有镜像仓库地址，大家部署的时候需要修改为自己的镜像。这里的pod镜像可以使用：pod-infrastructure 或者 pause 。pod-infrastructure镜像是Redhat制作的，大小接近80M，下载比较耗时，其实该镜像并不运行什么具体进程，推荐使用Google的pause镜像gcr.io/google_containers/pause-amd64:3.0，这个镜像只有300多K。

				启动kubelet
						systemctl daemon-reload
						systemctl enable kubelet
						systemctl start kubelet
						systemctl status kubelet
				通过 kublet 的 TLS 证书请求
						kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。
						查看未授权的 CSR 请求
							kubectl get csr
								NAME        AGE       REQUESTOR           CONDITION
								csr-2b308   4m        kubelet-bootstrap   Pending
							kubectl get nodes
								No resources found.
						通过 CSR 请求
							kubectl certificate approve csr-2b308
								certificatesigningrequest "csr-2b308" approved
							kubectl get nodes
							NAME           STATUS    AGE       VERSION
							192.168.55.243   Ready     1m       v1.9.2
				自动生成了 kubelet kubeconfig 文件和公私钥
						ls -l /etc/kubernetes/kubelet.kubeconfig
						ls -l /etc/kubernetes/ssl/kubelet*	
				
				假如你更新kubernetes的证书，只要没有更新token.csv，当重启kubelet后，该node就会自动加入到kuberentes集群中，而不会重新发送certificaterequest，也不需要在master节点上执行kubectl certificate approve操作。前提是不要删除node节点上的/etc/kubernetes/ssl/kubelet*和/etc/kubernetes/kubelet.kubeconfig文件。否则kubelet启动时会提示找不到证书而失败。
				注意： 如果启动kubelet的时候见到证书相关的报错，有个trick可以解决这个问题，可以将master节点上的~/.kube/config文件（该文件在安装kubectl命令行工具这一步中将会自动生成）拷贝到node节点的/etc/kubernetes/kubelet.kubeconfig位置，这样就不需要通过CSR，当kubelet启动后就会自动加入的集群中。
		
		3：配置 kube-proxy
				(1)	安装conntrack
						yum install -y conntrack-tools
				(2)	创建 kube-proxy 的service配置文件
						文件路径 vi /usr/lib/systemd/system/kube-proxy.service
						[Unit]
						Description=Kubernetes Kube-Proxy Server
						Documentation=https://github.com/GoogleCloudPlatform/kubernetes
						After=network.target

						[Service]
						EnvironmentFile=-/etc/kubernetes/config
						EnvironmentFile=-/etc/kubernetes/proxy
						ExecStart=/usr/bin/kube-proxy \
								$KUBE_LOGTOSTDERR \
								$KUBE_LOG_LEVEL \
								$KUBE_MASTER \
								$KUBE_PROXY_ARGS
						Restart=on-failure
						LimitNOFILE=65536

						[Install]
						WantedBy=multi-user.target
					
					kube-proxy配置文件 vi /etc/kubernetes/proxy。
						###
						# kubernetes proxy config

						# default config should be adequate

						# Add your own!
						KUBE_PROXY_ARGS="--bind-address=192.168.55.243 --hostname-override=192.168.55.243 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"

				说明：(1)	--hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；
					  (2)	kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；
					  (3)	--kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；
					  (4)	预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；
				
				(3)	启动 kube-proxy
						systemctl daemon-reload
						systemctl enable kube-proxy
						systemctl start kube-proxy
						systemctl status kube-proxy 
					注意：	如果启动失败，将master生成的kube-proxy.kubeconfig文件拷贝的该节点再次启动
							node2 节点 192.168.55.244 安装方式一样，只需要把相应配置文件里面的IP改为 192.168.55.244 即可
							新增节点的话只需要把master节点的证书拷贝到新主机，证书包括： /etc/kubernetes/bootstrap.kubeconfig; /etc/kubernetes/kube-proxy.kubeconfig;/etc/kubernetes/ssl/*.pem ，然后先安装flanneld，后照本文操作加入集群即可。

十：测试验证
		我们创建一个nginx的service试一下集群是否可用
				kubectl run nginx --replicas=2 --labels="run=load-balancer-example" --image=harbor.ztwltech.com/webservice/nginx:v1  --port=80
				kubectl expose deployment nginx --type=NodePort --name=example-service
				kubectl describe svc example-service
				curl 10.254.32.192:80	##在安装了kube-proxy的主机上获取
				192.168.55.243:32133 192.168.55.244:32133 ##kube-proxy的端口
				10.254.32.192:80 为集群内部地址，只有在安装了kube-proxy的节点上能够访问，访问这个地址时是做了负载均衡的

		删除此测试服务方法：
				kubectl delete deployment nginx; kubectl delete svc example-service
				
		至此，整个集群部署完成，由于没有服务调用，所以没有安装dns，也没有安装dashboard		

十一：安装coredns：
					apiVersion: v1
					kind: ServiceAccount
					metadata:
					  name: coredns
					  namespace: kube-system
					---
					apiVersion: rbac.authorization.k8s.io/v1beta1
					kind: ClusterRole
					metadata:
					  labels:
						kubernetes.io/bootstrapping: rbac-defaults
					  name: system:coredns
					rules:
					- apiGroups:
					  - ""
					  resources:
					  - endpoints
					  - services
					  - pods
					  - namespaces
					  verbs:
					  - list
					  - watch
					---
					apiVersion: rbac.authorization.k8s.io/v1beta1
					kind: ClusterRoleBinding
					metadata:
					  annotations:
						rbac.authorization.kubernetes.io/autoupdate: "true"
					  labels:
						kubernetes.io/bootstrapping: rbac-defaults
					  name: system:coredns
					roleRef:
					  apiGroup: rbac.authorization.k8s.io
					  kind: ClusterRole
					  name: system:coredns
					subjects:
					- kind: ServiceAccount
					  name: coredns
					  namespace: kube-system
					---
					apiVersion: v1
					kind: ConfigMap
					metadata:
					  name: coredns
					  namespace: kube-system
					data:
					  Corefile: |
						.:53 {
							errors
							health
							kubernetes cluster.local  10.254.0.0/16 {
							  pods insecure
							  upstream
							  fallthrough in-addr.arpa ip6.arpa
							}
							prometheus :9153
							proxy . /etc/resolv.conf
							cache 30
						}
					---
					apiVersion: extensions/v1beta1
					kind: Deployment
					metadata:
					  name: coredns
					  namespace: kube-system
					  labels:
						k8s-app: coredns
						kubernetes.io/name: "CoreDNS"
					spec:
					  replicas: 2
					  strategy:
						type: RollingUpdate
						rollingUpdate:
						  maxUnavailable: 1
					  selector:
						matchLabels:
						  k8s-app: coredns
					  template:
						metadata:
						  labels:
							k8s-app: coredns
						spec:
						  serviceAccountName: coredns
						  tolerations:
							- key: "CriticalAddonsOnly"
							  operator: "Exists"
						  containers:
						  - name: coredns
							image: coredns/coredns:1.1.1
							imagePullPolicy: IfNotPresent
							args: [ "-conf", "/etc/coredns/Corefile" ]
							volumeMounts:
							- name: config-volume
							  mountPath: /etc/coredns
							ports:
							- containerPort: 53
							  name: dns
							  protocol: UDP
							- containerPort: 53
							  name: dns-tcp
							  protocol: TCP
							- containerPort: 9153
							  name: metrics
							  protocol: TCP
							livenessProbe:
							  httpGet:
								path: /health
								port: 8080
								scheme: HTTP
							  initialDelaySeconds: 60
							  timeoutSeconds: 5
							  successThreshold: 1
							  failureThreshold: 5
						  dnsPolicy: Default
						  volumes:
							- name: config-volume
							  configMap:
								name: coredns
								items:
								- key: Corefile
								  path: Corefile
					---
					apiVersion: v1
					kind: Service
					metadata:
					  name: kube-dns
					  namespace: kube-system
					  annotations:
						prometheus.io/scrape: "true"
					  labels:
						k8s-app: coredns
						kubernetes.io/cluster-service: "true"
						kubernetes.io/name: "CoreDNS"
					spec:
					  selector:
						k8s-app: coredns
					  clusterIP: 10.254.0.2
					  ports:
					  - name: dns
						port: 53
						protocol: UDP
					  - name: dns-tcp
						port: 53
						protocol: TCP
			说明：corefile字段更换自己的 域名和网段
				  ClusterIP更换为自己的ip
			
	6: 检查 coredns 功能
		新建一个 Deployment						
						
			cat > my-nginx.yaml << EOF
			apiVersion: extensions/v1beta1
			kind: Deployment
			metadata:
			  name: my-nginx
			spec:
			  replicas: 2
			  template:
				metadata:
				  labels:
					run: my-nginx
				spec:
				  containers:
				  - name: my-nginx
					image: harbor.ztwltech.com/webservice/nginx:v1
					ports:
					- containerPort: 80
			EOF
			
			kubectl create -f my-nginx.yaml
			Export 该 Deployment, 生成 my-nginx 服务
			kubectl expose deploy my-nginx
			kubectl get services --all-namespaces |grep my-nginx
		进入kubernete生成的 my-nginx 服务的pods中
			kubectl get pods
			kubectl exec -it my-nginx-1108742923-1bpml /bin/bash
			root@my-nginx-1108742923-1bpml:~# cat /etc/resolv.conf
			
			ping my-nginx    
			ping kubernetes
			ping kube-dns.kube-system.svc.cluster.local
			
			
		service名称可以正常解析
		注意： 直接ping ClusterIP是ping不通的，ClusterIP是根据IPtables路由到服务的endpoint上，只有结合ClusterIP加端口才能访问到对应的服务。


		kubectl get pods --all-namespaces
		kubectl get services --all-namespaces
		kubectl get pods  -n kube-system | grep dashboard
			

				

		kubectl proxy --address='192.168.55.241' --port=8086 --accept-hosts='^*$'
		kubectl logs kubernetes-dashboard-7c74685c48-9qdpn -n kube-system



十二：安装helm
		1：首先需要安装helm客户端
			curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
			chmod 700 get_helm.sh
			./get_helm.sh
		2：创建tiller的serviceaccount和clusterrolebinding
			kubectl create serviceaccount --namespace kube-system tiller
			kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
		3：然后安装helm服务端tiller
			helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.0
		4：为应用程序设置serviceAccount：
			kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
		5：检查是否安装成功：
			$ kubectl -n kube-system get pods|grep tiller
			$ helm version  ##如果报错需要在node节点安装socat. yum install socat
			$ helm ls  ##helm未将KUBERNETES_MASTER环境变量从kubectl config中带入tiller的容器中，编辑deployment的环境变量强行写入
					 - name: KUBERNETES_MASTER
					   value: 192.168.55.250:8080
			

十三：安装nginx ingress
		1：下载相关的yaml文件并修改镜像地址
			https://github.com/kubernetes/ingress-nginx/tree/master/deploy
			https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md

		configmap.yaml  default-backend.yaml  namespace.yaml  publish-service-patch.yaml  rbac.yaml  tcp-services-configmap.yaml  with-rbac.yaml
		2：创建nginx-ingress   
			修改rabc.yaml 文件的      hostNework: true

			kubectl create -f namespace.yaml 
			kubectl create -f default-backend.yaml 
			kubectl create -f configmap.yaml
			kubectl create -f tcp-services-configmap.yaml 	
			kubectl create -f udp-services-configmap.yaml
		3: 测试 
			1：创建后端服务
				kubectl run --image=harbor.ztwltech.com/webservice/httpd:latest httpd --replicas=1
				kubectl expose deployment httpd --port=80
				kubectl run --image=harbor.ztwltech.com/webservice/nginx:v1 nginx --replicas=1
				kubectl expose deployment nginx --port=80
			2: 创建ingress
				apiVersion: extensions/v1beta1
				kind: Ingress
				metadata:
				  name: webservice-test
				spec:
				  rules:
				  - host: nginx.test.com
					http:
					  paths:
					  - backend:
						  serviceName: nginx
						  servicePort: 80
				  - host: httpd.test.com
					http:
					  paths:
					  - backend:
						  serviceName: httpd
						  servicePort: 80
				
		
附录：名词解释
		Secret: Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。
				Secret有三种类型：
						Service Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；
						Opaque：base64编码格式的Secret，用来存储密码、密钥等；
						kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。
				Opaque Secret:
						Opaque类型的数据是一个map类型，要求value是base64编码格式：
						$ echo -n "admin" | base64
						YWRtaW4=
						$ echo -n "1f2d1e2e67df" | base64
						MWYyZDFlMmU2N2Rm
				secrets.yml
						apiVersion: v1
						kind: Secret
						metadata:
						  name: mysecret
						type: Opaque
						data:
						  password: MWYyZDFlMmU2N2Rm
						  username: YWRtaW4=
					接着，就可以创建secret了：kubectl create -f secrets.yml。
					创建好secret之后，有两种方式来使用它：
							以Volume方式
							以环境变量方式

				Service account：是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的
				授权:	Service Account为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合RBAC来为Service Account鉴权：
